# Improving Language Understanding by Generative Pre-Training (GPT-1)

### Before reading

**ë…¼ë¬¸ ì œëª©ì„ ë³´ê³ , í•´ë‹¹ ëª¨ë¸ì´ ì–´ë–¤ ë°©ë²•ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ í• ì§€ ê°€ì„¤ì„ ì„¸ì›Œë´…ì‹œë‹¤.**

- NLU taskë¥¼ ***generative*** pre-trainingì„ í†µí•´ í•´ê²°í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì‹œí•˜ì§€ ì•Šì„ê¹Œ?
- Generative Pre-Training: Language Understandingì„ Generative Pre-Trainingìœ¼ë¡œ ê°œì„ í•œë‹¤â€¦ Generative modelì€ ë°ì´í„°ë¡œë¶€í„° í™•ë¥ ë¶„í¬ë¥¼ í•™ìŠµí•˜ê³ , í™•ë¥ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” sampleì„ ***ìƒì„±***í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œê³  ìˆë‹¤.
    
    <aside>
    ğŸ’¡ **Generative model**
    
    - Generation: ì…ë ¥ ë°ì´í„°ì˜ í™•ë¥ ë¶„í¬ $p(x)$ë¥¼ í•™ìŠµí•˜ê³ , ê·¸ëŸ¬í•œ í™•ë¥ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” sample ìƒì„±
    - Density estimation: íŒë³„í•˜ê³ ì í•˜ëŠ” ë°ì´í„°ë¥¼ ì…ë ¥í–ˆì„ ë•Œ, í™•ë¥ ê°’ì´ ë†’ê²Œ ë‚˜ì˜¤ë©´ ë§ëŠ” ë°ì´í„°
    - Explicit model: $P(x)$í™•ë¥  ë¶„í¬ê°€ ê°’ì´ í•­ìƒ 0ë³´ë‹¤ í¬ê³  ëª¨ë“  ê°€ëŠ¥í•œ ê°’ë“¤ë¡œ ì ë¶„í–ˆì„ ë•Œ 1ì´ ëœë‹¤ë©´ ì´ë¥¼ proper distributionì´ë¼ê³  ë¶€ë¥´ëŠ”ë°, ì´ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ëª¨ë¸ì„ Explicit modelì´ë¼ê³  í•œë‹¤.
    </aside>
    
- ì™œ Generative Pre-trainingì„ í•  ìƒê°ì„ í–ˆì„ê¹Œ?
    
    â†’ Unlabeled dataëŠ” ë§ì§€ë§Œ, íŠ¹ì • taskë¥¼ ìœ„í•œ labeled dataëŠ” ë¶€ì¡±í•´ì„œ, discriminatively trained modelì´ ì˜ ì‘ë™í•˜ê¸° ì–´ë ¤ìš´ ìƒí™©ì´ë‹¤.
    
    â†’ **Unlabeled dataë¥¼ í†µí•´ generative pre-trainingì„ ìˆ˜í–‰í•´ì„œ ë°ì´í„°ì— ëŒ€í•œ í™•ë¥ ë¶„í¬ë¥¼ ì¶©ë¶„íˆ í•™ìŠµí•˜ê³ , ê° taskì— ëŒ€í•´ discriminative fine-tuningì„ í•˜ì!**
    
    > **Abstract**
    > 
    > 
    > *Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.*
    > 
- GPTëŠ” Transformerì˜ decoderë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œë‹¤ê³  í•˜ëŠ”ë°, Transformerì˜ decoderëŠ” encodingëœ ì…ë ¥ ë¬¸ì¥ì˜ ì •ë³´ì™€ ì´ì „ì— ìƒì„±í•œ ë‹¨ì–´ì˜ ì •ë³´ë¥¼ í™œìš©í•´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìƒì„±í•œë‹¤.
- ì´ ë…¼ë¬¸ì´ Language Understandingì—ì„œ Generative pre-trainingì„ ì²˜ìŒìœ¼ë¡œ ì‚¬ìš©í•œ ê±´ê°€?
    
    â†’ ì´ì „ì—ë„ ì—¬ëŸ¬ ì‹œë„ê°€ ìˆì—ˆë‚˜ë³¸ë°(ì´ê±´ generative pre-trainingì„ ì‚¬ìš©í•œ ì‹œë„ë“¤ì„ ë§í•˜ëŠ” ê±´ì§€ ì•„ë‹Œì§€ ì°¾ì•„ë³´ì•„ì•¼ í•¨), ì—¬ê¸°ì„œëŠ” ì´ì „ ì‹œë„ì™€ ë‹¤ë¥´ê²Œ fine-tuning ê³¼ì •ì—ì„œ task-aware inputì„ ì‚¬ìš©í•´ì„œ ëª¨ë¸ êµ¬ì¡° ë³€í˜•ì„ ìµœì†Œí™”í•˜ë©´ì„œ íš¨ê³¼ì ìœ¼ë¡œ transfer learningì„ í•˜ê³ ì í•œë‹¤.
    
    > **Abstract**
    > 
    > 
    > *In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture.*
    > 

**ëª¨ë¸ì˜ ë©”ì¸ figureë¥¼ ë³´ê³  ê°ì„ ì¡ì•„ë´…ì‹œë‹¤.**

<img width="1337" alt="gpt_figure" src="https://github.com/ymnseol/tobigs-regular-session-assignments/assets/105059564/9089b3fd-6fc8-4d5f-bc22-547a50e26d56">


- Abstractì—ì„œ ê° taskë¥¼ ìœ„í•´ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤ê³  í•´ë„ ê¸°ì¡´ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ìµœì†Œí•œìœ¼ë¡œë§Œ ë³€í˜•í•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤ê³  í–ˆëŠ”ë°, ê·¸ ë§ëŒ€ë¡œ taskì— ë§ê²Œ input transformationì€ ë‹¤ë¥´ê²Œ í•˜ë˜ Transformer êµ¬ì¡°ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤.
- í’ë¶€í•œ unlabeled dataë¥¼ ì´ìš©í•´ generative pre-trainingì„ í•´ì„œ ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“¤ê³ , ì´í›„ ë‹¤ì–‘í•œ language understaning taskì—ì„œ ì´ ëª¨ë¸ì„ fine-tuningí•´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤!

**í•´ë‹¹ ëª¨ë¸ì„ êµ¬í˜„í•œ ì½”ë“œê°€ ìˆëŠ”ì§€ ì²´í¬í•´ë´…ì‹œë‹¤.**

https://github.com/openai/finetune-transformer-lm

**ì´ ë…¼ë¬¸ì—ì„œ í’€ê³ ì í•˜ëŠ” ë¬¸ì œê°€ ë¬´ì—‡ì¸ê°€ìš”? Taskë¥¼ ì •ì˜í•´ë´…ì‹œë‹¤.**

- Labeled dataê°€ ì ì€ language understanding taskì—ë„ ëª¨ë¸ì´ ì˜ ì‘ë™í•˜ê²Œ í•˜ê³ ì í•œë‹¤.
