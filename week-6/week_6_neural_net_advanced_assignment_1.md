# Neural Network

## Learning Rate Scheduler

### Cosine Annealing
$$
\eta_t = \eta_{min} + \frac{1}{2} (\eta_{max} - \eta_{min}) \bigg( 1 + \cos\bigg({\frac{T_{cur}}{T_{max}}\pi}\bigg) \bigg)
$$

* $\eta_{t}$:  step tì—ì„œì˜ learning rate
* $\eta_{max}$: learning rate ìµœëŒ“ê°’ (hyperparameter)
  * í•™ìŠµ ì‹œ ìµœëŒ€ learning rateëŠ” $\eta_{max}$ì…ë‹ˆë‹¤.
  * (Warmup ë“±ì„ í•˜ì§€ ì•Šìœ¼ë©´) initial learning rateì™€ ê°™ìŠµë‹ˆë‹¤.
* $\eta_{min}$: learning rate ìµœì†Ÿê°’ (hyperparameter)
  * í•™ìŠµì‹œ learning rateëŠ” $\eta_{min}$ ì•„ë˜ë¡œ ê°ì†Œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
* $T_{max}$: ê°ì†Œ / ì¦ê°€ ì£¼ê¸° (ë°˜ì£¼ê¸°)

![cosing-annealing](https://gaussian37.github.io/assets/img/dl/pytorch/lr_scheduler/7.png)

### Cosine Annealing with Warm Restarts

$$
\eta_t = \eta_{min} + \frac{1}{2} (\eta_{max} - \eta_{min}) \bigg( 1 + \cos\bigg({\frac{T_{cur}}{T_{i}}\pi}\bigg) \bigg)
$$

* $\eta_{t}$:  step tì—ì„œì˜ learning rate
* $\eta_{max}$: learning rate ìµœëŒ“ê°’ (hyperparameter)
  * í•™ìŠµ ì‹œ ìµœëŒ€ learning rateëŠ” $\eta_{max}$ì…ë‹ˆë‹¤.
  * (ì‹œì‘ ì‹œ warmup ë“±ì„ í•˜ì§€ ì•Šìœ¼ë©´) initial learning rateì™€ ê°™ìŠµë‹ˆë‹¤.
* $\eta_{min}$: learning rate ìµœì†Ÿê°’ (hyperparameter)
  * í•™ìŠµì‹œ learning rateëŠ” $\eta_{min}$ ì•„ë˜ë¡œ ê°ì†Œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
* $T_{i}$: ì£¼ê¸°

Learning rateê°€ ê°ì†Œí•˜ë‹¤ê°€ í˜„ì¬ epoch $T_{cur}$ì™€ $T_i$ê°€ ê°™ì•„ì§€ëŠ” ê²½ìš°, learning rateëŠ” $\eta_{min}$ì´ ë©ë‹ˆë‹¤. ì´í›„ $T_{cur}$ë¥¼ 0ìœ¼ë¡œ, learning rate $\eta_{t}$ë¥¼ $\eta_{max}$ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

ì´ˆê¸°ì— $\eta_{min}$ì—ì„œ ì‹œì‘í•˜ì—¬ learning rateë¥¼ ì¦ê°€ì‹œí‚¨ ë’¤ (warmup) ì£¼ê¸°ë¥¼ ë”°ë¥´ë„ë¡ í•  ìˆ˜ë„ ìˆê³ , ì£¼ê¸°ë¥¼ í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ ê¸¸ê²Œ ì„¤ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

## Optimizer

> *ğŸ¥² ê³¼ì œë¥¼ ì˜ëª» ì½ê³  optimizerì— ëŒ€í•´ ì„¤ëª…ì„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤...*

### Momentum
> SGD(Stochastic Gradient Descent)ì— **ê´€ì„± ê°œë… ì¶”ê°€í•´ step ë°©í–¥ ì¡°ì •**í•˜ê¸°

$$
v_{t+1} = \mu \cdot v_{t} + g_{t+1}
$$
$$
p_{t+1} = p_t - \eta \cdot v_{t+1}
$$

* $v$: velocity
* $\mu$: momentum
  * ì´ì „ì˜ ì´ë™ì„ ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
  * 0ì¸ ê²½ìš° SGDì™€ ë™ì¼í•©ë‹ˆë‹¤.
* $g$: gradient
* $p$: parameters
* $\eta$: learning rate

Gradientë¥¼ ì—…ë°ì´íŠ¸í•  ë•Œ, ì „ì˜ gradientì˜ ë°©í–¥ì„ ì–´ëŠ ì •ë„(momentum $\beta$ì˜ í¬ê¸°ë§Œí¼) ìœ ì§€í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ì „ gradientë¥¼ **ê´€ì„±**ê³¼ ê°™ì´ ì ìš©ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

**ğŸ’¡ Momentumì€ ì§€ë‚œ mini-batchì™€ëŠ” ë‹¤ë¥¸ ì´ë²ˆ mini-batchì— ëŒ€í•œ í•™ìŠµì—ì„œë„, ì´ì „ mini-batchì˜ ì •ë³´ ì—­ì‹œ í™œìš©í•˜ë„ë¡ í•©ë‹ˆë‹¤.**

ğŸ‘ ê¸‰ê²©í•œ ë°©í–¥ì˜ ë³€í™”ë¥¼ $\mu \cdot v_{t}$ê°€ ì™„í™”í•´ì£¼ê¸° ë•Œë¬¸ì—, SGDì— ë¹„í•´ ìˆ˜ë ´ ì†ë„ê°€ ë¹ ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ‘ Velocityê°€ gradientë³´ë‹¤ í° ê²½ìš°, ê´€ì„±ì— ì˜í•´ ìµœì ì˜ ì§€ì ì„ ì§€ë‚˜ì³ë²„ë¦¬ëŠ” í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Adagrad(Adaptive Gradient)
> SGDì—ì„œ **loss functionì˜ ë³€í™”ì— ë”°ë¼ í•™ìŠµë¥ ì„ ê²°ì •**í•˜ì—¬ ë°˜ì˜í•˜ê¸°

$$
G_{t+1} = G_{t} + (g_{t+1})^2
$$
$$
p_{t+1} = p_{t} - \frac{\eta}{\sqrt{G_{t+1}} + \epsilon} g_{t+1}
$$

* $p$: parameters
* $\eta$: initial learning rate
* $g$: gradient
* $G$: $g^2$ì˜ í•©
* $\epsilon$: ë¶„ëª¨ê°€ 0ì´ ë˜ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•œ ê°’

Parameterê°€ ì–¼ë§ˆë‚˜ ë³€í–ˆëŠ”ì§€ë¥¼ ë”°ì ¸, ë§ì´ ë³€í™”ëœ parameterì˜ ê²½ìš° ë” ì ê²Œ, ì ê²Œ ë³€í™”ëœ parameterì˜ ê²½ìš° ë” ë§ì´ ë³€í™”ì‹œí‚¤ë„ë¡ learning rateë¥¼ ë³€í™”ì‹œí‚µë‹ˆë‹¤.

ğŸ‘ Learning rateê°€ ì í•©í•˜ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” ë¬¸ì œ(ë„ˆë¬´ í¬ë©´ ë°œì‚°í•˜ëŠ” ë“±)ë¥¼ learning rateë¥¼ ì¡°ì •í•˜ì—¬ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ‘ Gradientì˜ ì œê³±ì˜ í•© $G$ëŠ” í•™ìŠµì„ í• ìˆ˜ë¡ ì»¤ì ¸ì„œ, **learning rateì´ 0ì— ìˆ˜ë ´í•´ í•™ìŠµì´ ë©ˆì¶”ëŠ” í˜„ìƒ**ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### RMSProp(Root Mean Square Propagation)
> Adagradì—ì„œ ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê· ì„ ì´ìš©í•´, ìµœì‹ ì˜ ê¸°ìš¸ê¸°ë¥¼ ë” í¬ê²Œ ë°˜ì˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.

$$
G_{t+1} = \alpha \cdot G_{t} - (1 - \alpha) (g_{t+1})^2
$$
$$
p_{t+1} = p_{t} - \frac{\eta}{\sqrt{G_{t+1}} + \epsilon} g_{t+1}
$$

* $G$: gradientì˜ ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê· 
* $p$: parameters
* $\eta$: initial learning rate
* $g$: gradient
* $\epsilon$: ë¶„ëª¨ê°€ 0ì´ ë˜ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•œ ê°’

$G$ë¥¼ ê³„ì‚°í•  ë•Œ, ìµœê·¼ì˜ gradientë¥¼ ë” í¬ê²Œ ë°˜ì˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.

ğŸ‘ Adagradì—ì„œ ë°œìƒí–ˆë˜ í•™ìŠµì´ ë©ˆì¶”ëŠ” í˜„ìƒì„ ì™„í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Adam(Adaptive Moment Estimation)
> Momentum + RMSProp

$$
m_{t+1} = \beta_{1} \cdot m_{t} + (1 - \beta_{1}) g_{t}
$$
$$
v_{t+1} = \beta_{2} \cdot v_{t} + (1 - \beta_2) (g_{t+1})^2
$$
$$
\hat{m}_{t+1} = \frac{m_{t+1}}{1 - (\beta_{1})^{t+1}} \\
$$
$$
\hat{v}_{t+1} = \frac{v_{t+1}}{1 - (\beta_{2})^{t+1}}
$$
$$
p_{t+1} = p_{t} - \eta \frac{\hat{m}_{t+1}}{\sqrt{\hat{v}_{t+1}} + \epsilon}
$$

* $m$: first moment
* $v$: second moment
* $\hat{m}$: bias-corrected first moment
  * Momentumì˜ velocity ì•„ì´ë””ì–´ì— ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê·  ê°œë…ì„ ì ìš©í•©ë‹ˆë‹¤.
* $\hat{v}$: bias-corrected second moment
  * RMSPropì˜ gradientì˜ ì œê³±ì˜ ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê·  $G$ ì•„ì´ë””ì–´ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
* $\beta_{1}$: first momentì˜ decay rate
* $\beta_{2}$: second momentì˜ decay rate
* $\eta$: learning rate
* $\epsilon$: ë¶„ëª¨ê°€ 0ì´ ë˜ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•œ ê°’
* $p$: parameters

í•™ìŠµì„ ì‹œì‘í•  ë•Œ,

* $m_0 = 0$
* $v_0 = 0$
* $\beta_{1} = 0.9$
* $\beta_{2} = 0.999$

ë¡œ ì´ˆê¸°í™”ë˜ì–´ìˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë©´,

$$
m_{1} \\
= \beta_{1} \cdot m_{0} + (1 - \beta_{1}) g_{1} \\
= 0 + 0.1 \cdot g_{1}
$$
$$
v_{1} \\
= \beta_{2} \cdot v_{0} + (1 - \beta_2) (g_{1})^2 \\
= 0 + 0.001 \cdot (g_{1})^{2}
$$
ë¡œ, momentê°€ 0ì— ë„ˆë¬´ ê°€ê¹ë„ë¡ ì¹˜ìš°ì³ì§„ì±„ë¡œ ì‹œì‘í•˜ê²Œ ë©ë‹ˆë‹¤.

$\beta_{1}$, $\beta_{2}$ëŠ” 0 ì´ìƒ 1 ë¯¸ë§Œì¸ ì‹¤ìˆ˜ê°’ìœ¼ë¡œ, bias-corrected first moment $\hat{m}$ê³¼ bias-corrected second moment $\hat{v}$ì˜ ë¶„ëª¨ëŠ” ê°’ì´ ì ì  ì»¤ì ¸ 1ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤. í•™ìŠµ ì´ˆë°˜ì— bias correctionì„ ì–´ëŠ ì •ë„ ìˆ˜í–‰í•˜ê³  ë‚˜ë©´, bias correctionì€ íš¨ê³¼ê°€ ë¯¸ë¯¸í•´ì§€ê²Œ ë©ë‹ˆë‹¤. (Bias correctionì„ ì ìš©í•˜ëŠ” ê²ƒê³¼ ì ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ë¹„ìŠ·í•´ì§‘ë‹ˆë‹¤.)

**ğŸ’¡ Adamì€ Momentumì˜ velocityì™€ RMSPropì˜ ì§€ìˆ˜ê°€ì¤‘ì´ë™í‰ê· , bias correctionì„ í†µí•´ ì²˜ìŒë¶€í„° ì ì ˆí•œ ê°’ìœ¼ë¡œ í•™ìŠµì„ ì˜ ì‹œì‘í•˜ë„ë¡ í•˜ê³ , step ë°©í–¥ ë° ì‚¬ì´ì¦ˆ ì—­ì‹œ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì— ë°˜ì˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.**

## Generalization Performance

### Ensemble

> **Bagging(Bootstrapping Aggregating)**
> > **Bootstrapping**
> >
> > ë³µì›ì¶”ì¶œì„ ì‚¬ìš©í•˜ëŠ” í…ŒìŠ¤íŠ¸, í‰ê°€ì§€í‘œ ë“±ì„ í†µí‹€ì–´ bootstrappingì´ë¼ê³  í•©ë‹ˆë‹¤.
>
> **Method**
> 
> Baggingì€
> 1. í•™ìŠµ ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ê³ ì •ë˜ì–´ ìˆì„ ë•Œ(ì¶”ê°€ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤ê±°ë‚˜ í•˜ëŠ” ìƒí™©ì„ ë°°ì œ)
> 2. í•™ìŠµ ë°ì´í„°ë¥¼ bootstrappingì„ í†µí•´ **ì—¬ëŸ¬ í‘œë³¸ë°ì´í„°ì…‹**ìœ¼ë¡œ êµ¬ì„±í•´ì„œ
> 3. ê° ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ **ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸**ì„ ë§Œë“¤ê³ 
> 4. ê·¸ë ‡ê²Œ ë‚˜ì˜¨ ì—¬ëŸ¬ ì¶œë ¥ì„ ìˆ˜í•©(í‰ê· ë‚´ê¸°, ì¶”ê°€ classifier ì“°ê¸° ë“±)í•˜ì—¬ ìµœì¢… ì¶œë ¥ì„ ê²°ì •í•˜ëŠ”
> 
> ë°©ë²•ì…ë‹ˆë‹¤.
> 
> **Objectives**
> 
> Baggingì˜ ëª©ì ì€ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ í™•ë³´í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ê²ƒì…ë‹ˆë‹¤.
> 
> **ğŸ’¡ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ í™•ë³´í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•œ baggingì€ varianceë¥¼ ë‚®ì¶”ëŠ” ë°©ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!**

### Early Stopping

Validation loss ë˜ëŠ” ë‹¤ë¥¸ metricì„ ì°¸ê³ í•˜ì—¬, ì´ˆê¸°ì— ì„¤ì •í•œ í•™ìŠµ epoch / steps ë³´ë‹¤ ë” ë¹¨ë¦¬, ëª¨ë¸ì´ overfittingë˜ê¸° ì „ í•™ìŠµì„ ë©ˆì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```Python
...
if val_loss >= best_loss:
    current_patience += 1
    if current_patience >= patience:
        break
else:
    best_loss = val_loss
    current_patience = 0
...
```

### Label Smoothing

íŠ¹íˆ ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ”, ëª¨ë¸ì´ ground truthë¡œ ë¶„ë¥˜í•  í™•ë¥ ì„ 1ì— ê°€ê¹ê²Œ ë„ì¶œí•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²½ìš°, ëª¨ë¸ì´ ì§€ë‚˜ì¹œ í™•ì‹ ì„ ê°€ì§€ê²Œ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(í•™ìŠµ ë°ì´í„°ì˜ ë¶„í¬ì—ë§Œ ê³¼ë„í•˜ê²Œ ì¼ì¹˜í•˜ëŠ” ë¶„í¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

í•™ìŠµ ë°ì´í„°ì˜ ì •ë‹µ labelì´ ì•„ë‹Œ ë‹¤ë¥¸ labelì—ë„ ì¼ì •í•œ í™•ë¥ ì„ ë¶€ì—¬í•œ ë¶„í¬ë¥¼ í•™ìŠµì‹œí‚¤ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## References

[Pytorch Learning Rate Scheduler (ëŸ¬ë‹ ë ˆì´íŠ¸ ìŠ¤ì¼€ì¥´ëŸ¬) ì •ë¦¬](https://gaussian37.github.io/dl-pytorch-lr_scheduler/#cosineannealinglr-1)

[Why is it important to include a bias correction term for the Adam optimizer for Deep Learning?](https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for)

[ë”¥ëŸ¬ë‹ ìš©ì–´ì •ë¦¬, RMSProp, Adam ì„¤ëª…](https://light-tree.tistory.com/141)

[Optimization(ìµœì í™” ì•Œê³ ë¦¬ì¦˜) : Mini-batch/Momentum/RMSprop/Adam](https://junstar92.tistory.com/81)

[ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION](https://arxiv.org/pdf/1412.6980v9.pdf)

[Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567.pdf)
